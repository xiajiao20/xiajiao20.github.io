# 56班第二次答疑课

同学们好啊，今天我们要学习的是爬虫的内容。老师今天教你们怎么分辨静态数据还是动态数据。

## 安装第三方库

在那之前老师确认一下我们的库是都安装好了吧，有没有同学是电脑刚到还没来的及安装库的？

我们安装库是有固定格式的，大家可以记一下

```
pip install <库名> -i <镜像源>
```

这个库名是可以填多个的，中间用空格分隔就可以了，就比如我们今天要使用的两个库可以这样写

```
pip install requests beautifulsoup4 -i https://mirrors.aliyun.com/pypi/simple/
```

这个命令是需要在终端运行的

## 分析静态还是动态

欧克，现在进入正题了。

我先带大家回顾一下，我们爬虫的一般实现步骤

第一步是不是找数据？找数据，数据在哪里啊，数据一般来说在我们的一个网页的一个地址中，一般来说我们把这个地址叫做URL，也就是说我们第一步是要`找数据对应的url`。有了这个地址之后呢，那我们的第二步干什么。第二步是不是`对URL发送请求`获取当前URL返回给我们的数据。有了返回的数据，我们是不是就可以对`数据进行解析`。基于我们获取到数据的前提情况下，对于我们想要的数据进行解析提取。通过这些步骤我们是不是得到了需要的数据，那么有了数据之后我们是不是对数据进行一个保存操作，我们的第四步就是`数据保存`

```
爬虫的一般步骤
1.找数据对应的URL(分析网页性质获得)
2.对URL发送请求
3.数据解析
4.数据保存
```

这就是我们爬虫的一般步骤，我们今天的案例也是按照这个来。教大家一步一步的实现。思路没问题的话，大家在公屏敲个1。

我们第一步是不是找到数据对应的URL，怎么去找呢，我们先来看当前这个网页  房天下：https://cs.zu.fang.com/，在当前网页我们是能看到我们想要的数据的，比如说（打开网页，截图圈需要爬取的数据）这些数据是不是我们在网页中能看到的，接下来就要分析我们看到的这些数据对应的地址是哪一个。怎么去分析呢，我们先点击到网页空白处，右键查看网页源代码，它会弹出一个新的浏览器页面，我们ctrl+f进行搜索，发现能搜索到数据，在这个窗口中展示的数据其实就是我们这个网页的源数据。这种能搜索到数据的就是静态页面

重点来了，如果说我们能在网页源代码中搜索到想要的数据，那个数据的地址就是我们当前页面中的地址导航栏的链接。如果我们在网页源代码中搜索不到的，他就是一个动态数据。那么动态数据的话就需要我们去抓包进行分析。

什么是抓包？抓包就是获取我们本地电脑与远端服务器通信时候所传递的数据包。

怎么进行抓包呢。我们在空白处鼠标右键点击检查，我们就会弹出来一个浏览器开发者工具，这个开发者工具给我们提供了一个抓包的工具，也就是这个 网络，英文是network。我们现在这个是静态页面的我就不进行抓包了，我们用这个网站进行演示 异步社区：https://www.epubit.com/books 



1.URL参数

  URL 地址栏的参数如果会变化就是静态页面，反之这种一直是同一个的都是动态数据

2.网页源代码

  鼠标右键，选择“查看网页源代码"，这时候它会打开一个新页面，我们在新页面ctrl+f进行搜索，如果能搜索到数据的也是静态页面，反之就是动态的



这个分析网页数据还有一个作用，可以帮我们决定使用什么进行数据解析，如果是静态页面，他的数据都在HTML 页面源码中这时候我们就要使用bs4进行数据提取。如果是动态数据，他们返回的一般是json数据，我们直接`response.json()`

将json数据转换为 python 字典数据 。按照字典取值的方法来就可以了。



## 房天下

现在回到房天下这里来

```python
import requests
from bs4 import beautifulsoup
"""
爬虫的一般步骤
1.找数据对应的URL(分析网页性质获得)
2.对URL发送请求
3.数据解析
4.数据保存
"""
# 1.找数据对应的URL(分析网页性质获得)
url = 'https://cs.zu.fang.com/'
```

```python
# 2. 对 URL 发送请求
"""
从服务器获取数据的是 GET 请求  
    - GET 请求会将参数拼接在 URL 后面（?key=value&key2=value2）
    - 一般用于获取数据，而不是修改服务器数据
    - 请求参数长度有限（浏览器和服务器会限制）
    例如：
        访问网页：http://example.com/index.html
        搜索内容：http://example.com/search?keyword=python

提交数据到服务器的是 POST 请求  
    - POST 请求会将数据放到请求体（body）中，不会直接显示在 URL 上
    - 一般用于提交表单、登录、上传文件等
    例如：
        登录接口：http://example.com/login
        上传文件接口：http://example.com/upload
"""
response = requests.get(url)
print(response) # 返回了尖括号，在python中这种尖括号就是对象，所以它返回了一个response对象
# 自定义 Person 类
class Person:
    def __init__(self, name, age):
        self.name = name
        self.age = age
        
    # 控制 print 输出的样子  __repr__ 读作 瑞（ruì） + 普 + 轻声 儿
    def __repr__(self):
        return f"<Person name={self.name} age={self.age}>"

a = Person("Alice", 18)
print(a)  # <Person name=Alice age=18>

print(reponse.text)# 提取当前这个对象里面的文本内容啊
```

```python
# 3.数据解析
soup = BeautifulSoup(response.text,'html.parser') # parser 读 怕色儿 要快
dls = soup.find('div',class_='houseList').find_all('dl')
print(len(dls))
for dl in dls:
    title = dl.find('dd').find('p',class_='title').find('a').text
    print(title)
    info = dl.find('dd').find('p',class_='font15 mt12 bold').text
    print(info)
    address = dl.find('dd').find('p',class_='gray6 mt12').text
    print(address)
    price = dl.find('dd').find('div',class_='moreInfo').find('span',class_='price').text
    print(price)
    imgURL = 'https:'+dl.find('dt').find('img')['data-original']
    print(imgURL)
```



- 
- **User-Agent**
  - 浏览器身份标识，伪装成一个浏览器用户
- **`cookies`**
  - 保存登录状态或反爬验证信息
- `referer`
  - 用于标识当前请求是从哪个页面跳转过来的





现在我们成功获取了房天下的数据。我们再来看下房天下和异步社区的请求方式。我们发现这两个网站的都是GET请求。大家还记得我们另外一个POST请求吗。第9节课的时候，那个测试网站崩了，然后流星老师就没讲POST请求了，今天就由我给大家补充一下

| 项目             | 内容说明                               |
| ---------------- | -------------------------------------- |
| 请求方式         | `POST`                                 |
| 参数位置         | 请求体（body）中                       |
| 数据位置         | 服务器动态返回                         |
| 判断方法         | 提交表单、搜索或登录动作触发 POST 请求 |
| 是否能查看源代码 | 看不到提交的数据                       |
| 返回格式         | 页面/跳转/响应 HTML                    |
| 示例行为         | 登录、搜索、提交表单                   |
| 抓取方式         | `requests.post(url, data=payload)`     |

现在我们来看一个POST请求的案例

## 阿里云招聘

现在来讲阿里云招聘，通过这个例子给你们讲解POST请求

https://careers.aliyun.com/off-campus/position-list

```python
# 导入 requests 库，用于发送 HTTP 请求
import requests
# 导入 json 库，用于处理 JSON 数据
import json
# 导入 pandas 库，用于数据处理和写入 Excel 文件
import pandas as pd

# 定义请求头，模拟浏览器请求，避免被服务器拒绝
headers = {
    "accept": "application/json, text/plain, */*",
    "accept-language": "zh-CN,zh;q=0.9",
    "bx-v": "2.5.11",
    "content-type": "application/json",
    "origin": "https://careers.aliyun.com",
    "priority": "u=1, i",
    "referer": "https://careers.aliyun.com/off-campus/position-list?lang=zh&search=%E5%A4%A7%E6%A8%A1%E5%9E%8B",
    "sec-ch-ua": "\"Not)A;Brand\";v=\"8\", \"Chromium\";v=\"138\", \"Google Chrome\";v=\"138\"",
    "sec-ch-ua-mobile": "?0",
    "sec-ch-ua-platform": "\"Windows\"",
    "sec-fetch-dest": "empty",
    "sec-fetch-mode": "cors",
    "sec-fetch-site": "same-origin",
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36"
}
# 定义请求所需的 cookies，用于维持会话状态
cookies = {
    "cna": "Yb30INtD20UBASQIhFOfOac2",
    "_rb_id": "6254d19d52194dbdbf8cbce17f32f4e5",
    "XSRF-TOKEN": "266bd789-9148-495f-baa4-23949b9af26c",
    "SESSION": "MEIyMUFGMUU4NkE5MkU5MTBENzZCQkJEQjBDNTY3QUQ=",
    "arms_uid": "e04a3ed9-2146-44af-bf8e-8fa37428cbca",
    "tfstk": "glix2t_jIQAc7k1VHfYlIkePHU9kKUD4NjkCj5Vc5bh-1feioKzg14hiNxX0nA086bG8GSvwQ0FtsbFGoSqjCfHsMGD0mAmJBjDNjfHw3RNs1fFioUxn3xrafBq9xHDqb18w_fUbfu1SURV1GU4fwJSrUBAHx3YfFldvtffRq0M7Q7wbCGw6eTwgN16j1Sa5NJeUf5Gs17s7CRb_ClssFTNzClNs1l98V7yTf5G_fLHSMkBYstNOXqC7v1Tv8pjNb0eYk5IggGHn4M48OjPRfBi4H6PIh7I1fWdGZ5H-nIIEEooo9JcDVihKh2GsRbtRDzujecUEbs97fPgo-r3JGGFomRUQfzB1fYEbaPw4AH18U4Mm5mlABGwqmD4TTzp1bPqSxygIMOJEFows_yoMTiPjCvoE-kKOVoU14EinvwPctWeGlLpR7NzbUhlR7-_m_fO0eWvlrN7ar8y8tLdd7NzbU8FHE9bN7z2P.",
    "isg": "BNPTAJdqv2HmcXNC92E19g7BYlf9iGdKn1fFlYXxGPIFBPemC1sOmv4SPnRqpL9C"
}
# 定义请求的目标 URL，即阿里云招聘岗位搜索接口
url = "https://careers.aliyun.com/position/search"
# 定义请求参数，包含 CSRF 令牌
params = {
    "_csrf": "266bd789-9148-495f-baa4-23949b9af26c"
}
# 初始化一个空字典，用于存储各岗位的数据框，键为岗位名称，值为对应的数据框
data_frames = {}

# 进入循环，持续获取用户输入，直到用户输入 'exit' 退出程序
while True:
    # 提示用户输入要搜索的岗位名称，并接收输入
    key = input('请输入要搜索的岗位(输入exit退出程序)：')
    # 判断用户是否输入 'exit'，若是则跳出循环
    if key == 'exit':
        break
    # 定义请求体数据，包含搜索的相关参数
    data = {
        "channel": "group_official_site",
        "language": "zh",
        "batchId": "",
        "categories": "",
        "deptCodes": [],
        "key": key,
        "pageIndex": 1,
        "pageSize": 30,
        "regions": "",
        "subCategories": ""
    }
    # 将请求体数据转换为 JSON 字符串，去除多余空格
    data = json.dumps(data, separators=(',', ':'))
    # 发送 POST 请求，获取搜索结果
    response = requests.post(url, headers=headers, cookies=cookies, params=params, data=data)
    # 将响应内容解析为 JSON 格式
    res = response.json()
    # 初始化一个空列表，用于存储当前搜索岗位的所有职位信息
    job_data = []
    # 判断搜索结果中是否存在职位数据
    if res['content']['datas']:
        # 遍历每个职位数据
        for i in res['content']['datas']:
            # 获取职位名称
            jobName = i['name']
            # 获取职位描述
            description = i['description']
            # 获取职位要求
            requirement = i['requirement']
            # 拼接职位详情链接
            jobUrl = 'https://careers.aliyun.com'+i['positionUrl']
            # 拼接职位工作地点
            workLocation = ','.join(i['workLocations'])
            # 打印职位信息
            print(jobName)
            print(description)
            print(requirement)
            print(jobUrl)
            print(workLocation)
            print('*'*30)
            # 将当前职位信息添加到 job_data 列表中
            job_data.append([jobName, description, requirement, jobUrl, workLocation])
            # 将当前搜索岗位的数据框添加到 data_frames 字典中
        data_frames[key] = job_data
    else:
        # 若搜索结果中无职位数据，提示用户暂无在招职位
        print('暂无在招职位')

# 判断 data_frames 字典是否为空，若不为空则将数据写入 Excel 文件
if data_frames:
    with pd.ExcelWriter('aliyun.xlsx') as writer:
        # 遍历 data_frames 字典，将每个岗位的数据写入对应的 sheet 中
        for sheet_name, data in data_frames.items():
            # 将 job_data 列表转换为 pandas 的 DataFrame 对象
            df = pd.DataFrame(data, columns=['岗位名称', '岗位描述', '岗位要求', '详情连接', '工作地点'])
            # 将数据框写入 Excel 文件的指定 sheet 中，不包含索引列
            df.to_excel(writer, sheet_name=sheet_name, index=False)
```

在 Pandas 里使用 pd.ExcelWriter 主要是为了方便向同一个 Excel 文件的不同 sheet 中写入对应的表格数据，在处理含有多个 sheet 的 Excel 文件时非常方便 3 。使用时，需要先创建一个 writer 对象，借助 to_excel() 方法将不同的数据框及其对应的 sheet 名称写入该对象，全部表格写入完成后，使用 save() 方法把内容写入实体 Excel 文件。

如果不使用 ExcelWriter ，仅处理单个 sheet 时，可直接使用 DataFrame.to_excel() 方法。不过该方法一次只能写入一个 sheet，若要写入多个 sheet，就需要多次调用该方法，每次写入都会覆盖之前的内容，无法将多个 sheet 保存到同一个文件中。所以，若需向同一个 Excel 文件的多个 sheet 写入数据， ExcelWriter 是更合适的选择。



**`requests`**

- 用于发送 HTTP 请求，这里用 `requests.post()` 发送 POST 请求到阿里云招聘接口。
- 通过 `headers` 和 `cookies` 模拟浏览器行为，避免反爬或权限限制。
- 使用 `params` 传递 URL 查询参数（如 `_csrf`），`data` 传递请求体（岗位搜索条件）。

- **`json`**
  - `json.dumps(data, separators=(',', ':'))` 将 Python 字典转换成 JSON 字符串，且去掉多余空格，使请求更接近浏览器发送的格式。
  - `response.json()` 将返回的 JSON 格式字符串直接解析成 Python 字典。
- **`openpyxl`**
  - 用于创建和操作 Excel 文件。
  - `Workbook()` 创建一个新的 Excel 工作簿。
  - `wk.create_sheet(title=key)` 为每次搜索创建一个新的工作表（sheet）。
  - `ws.append([...])` 在 sheet 末尾追加一行数据。
  - `wk.save('aliyun.xlsx')` 保存文件。

------

### **Excel 表格操作**

- **删除默认 sheet**：

  ```python
  if 'Sheet' in wk.sheetnames:
      del wk['Sheet']
  ```

  避免生成无意义的默认空表。

- **动态创建 sheet**：
   每次输入不同的岗位关键字，都会新建一个同名 sheet 保存搜索结果。

- **写入数据**：
   先写标题行（表头），再写多行岗位信息





- GET：适合抓静态网页内容，如新闻、博客、商品页。
- POST：适合抓表单提交后页面，如搜索结果、登录后跳转。
- API：适合抓分页数据、评论列表、异步加载的内容。

















## **openpyxl 常用功能速查表**

### 1. **创建与保存 Excel**

```python
from openpyxl import Workbook

# 创建新工作簿
wb = Workbook()

# 删除默认 sheet
if 'Sheet' in wb.sheetnames:
    del wb['Sheet']

# 创建新 sheet
ws = wb.create_sheet(title="数据表")

# 保存文件
wb.save("example.xlsx")
```

------

### 2. **读取 Excel**

```python
from openpyxl import load_workbook

# 打开已存在的工作簿
wb = load_workbook("example.xlsx")

# 获取所有 sheet 名称
print(wb.sheetnames)

# 选择指定 sheet
ws = wb["数据表"]

# 读取单元格内容
print(ws["A1"].value)       # 按 Excel 格式引用
print(ws.cell(row=1, column=1).value)  # 按行列索引引用

# 获取最大行列
print(ws.max_row, ws.max_column)
```

------

### 3. **写入数据**

```python
# 写入单个单元格
ws["A1"] = "姓名"
ws.cell(row=1, column=2, value="年龄")

# 写入一行数据
ws.append(["张三", 25, "北京"])

# 批量写入
data = [
    ["李四", 30, "上海"],
    ["王五", 28, "广州"]
]
for row in data:
    ws.append(row)
```

------

### 4. **格式化单元格**

```python
from openpyxl.styles import Font, Alignment, PatternFill, Border, Side

# 字体
ws["A1"].font = Font(name="微软雅黑", size=14, bold=True, color="FF0000")

# 对齐方式
ws["A1"].alignment = Alignment(horizontal="center", vertical="center", wrap_text=True)

# 背景填充
ws["A1"].fill = PatternFill(fill_type="solid", fgColor="FFFF00")

# 边框
thin = Side(border_style="thin", color="000000")
ws["A1"].border = Border(left=thin, right=thin, top=thin, bottom=thin)

# 设置列宽 & 行高
ws.column_dimensions["A"].width = 20
ws.row_dimensions[1].height = 30
```

------

### 5. **合并/拆分单元格**

```python
# 合并单元格
ws.merge_cells("A1:C1")
# 拆分单元格
ws.unmerge_cells("A1:C1")
```

------

### 6. **操作行列**

```python
# 插入行/列
ws.insert_rows(2)   # 在第2行前插入一行
ws.insert_cols(2)   # 在第2列前插入一列

# 删除行/列
ws.delete_rows(2, 1)  # 从第2行开始删除1行
ws.delete_cols(2, 1)  # 从第2列开始删除1列
```

------

### 7. **遍历数据**

```python
# 遍历所有行
for row in ws.iter_rows(values_only=True):
    print(row)

# 遍历所有列
for col in ws.iter_cols(values_only=True):
    print(col)
```

------

### 8. **保存并关闭**

```python
wb.save("example.xlsx")
wb.close()
```

------

✅ **总结**

- `Workbook()` → 创建新 Excel
- `load_workbook()` → 读取已有 Excel
- `append()` → 快速写入一行
- `ws["A1"]` / `ws.cell()` → 访问单元格
- `styles` 模块 → 格式化字体、颜色、对齐等
- `merge_cells()` / `unmerge_cells()` → 合并拆分
- `iter_rows()` / `iter_cols()` → 遍历表格
